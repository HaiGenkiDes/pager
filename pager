#!/usr/bin/env python

#stdlib imports
import argparse
import configparser
import os.path
import sys
import pprint
import locale
import json
import logging
import re
import glob
import traceback
import io

#third party imports
import pandas as pd
import numpy as np
from mapio.shake import getHeaderData
from impactutils.comcat.query import ComCatInfo

#local imports
from losspager.models.exposure import Exposure
from losspager.models.econexposure import EconExposure
from losspager.models.emploss import EmpiricalLoss
from losspager.models.semimodel import SemiEmpiricalFatality
from losspager.models.growth import PopulationGrowth
from losspager.utils.country import Country
from losspager.utils.eventpath import get_event_folder
from losspager.utils.logger import PagerLogger
from losspager.utils.admin import PagerAdmin
from losspager.onepager.comment import get_impact_comments
from losspager.onepager.comment import get_structure_comment
from losspager.onepager.comment import get_secondary_comment
from losspager.onepager.comment import get_historical_comment
from losspager.io.pagerdata import PagerData
from losspager.vis.impactscale import drawImpactScale
from losspager.vis.contourmap2 import draw_contour
from losspager.utils.config import read_config

COUNTRY = Country()
TIMEFMT = '%Y-%m-%d %H:%M:%S'
TSUNAMI_MAG_THRESH = 7.3

def _get_pop_year(event_year,popyears):
    pop_year = None
    tmin = 10000000
    popfile = None
    for popdict in popyears:
        popyear = popdict['population_year']
        popgrid = popdict['population_grid']
        if not os.path.isfile(popgrid):
            print('Population grid file %s does not exist.' % popgrid)
            sys.exit(1)
        if abs(popyear-event_year) < tmin:
            tmin = abs(popyear-event_year)
            pop_year = popyear
            popfile = popgrid
    return (pop_year,popfile)

def get_pager_version(eventfolder):
    pager_version = 1
    if not os.path.isdir(eventfolder):
        os.makedirs(eventfolder)
        last_version = 0
    else:
        allfolders = glob.glob(os.path.join(eventfolder,'version.*'))
        allfolders.sort()
        if len(allfolders):
            base,last_folder = os.path.split(allfolders[-1])
            last_version = int(re.findall('\d+',last_folder)[0])
        else:
            last_version = 0
    return last_version + 1

def _draw_probs(fatmodel,fatdict,ecomodel,ecodict,version_folder):
    fatG = fatmodel.getCombinedG(fatdict)
    fat_probs = fatmodel.getProbabilities(fatdict,fatG)
    fat_figure = drawImpactScale(fat_probs,'fatality')

    ecoG = ecomodel.getCombinedG(ecodict)
    eco_probs = ecomodel.getProbabilities(ecodict,ecoG)
    eco_figure = drawImpactScale(eco_probs,'economic')

    fat_probs_file = os.path.join(version_folder,'alertfatal.pdf')
    eco_probs_file = os.path.join(version_folder,'alertecon.pdf')
    fat_figure.savefig(fat_probs_file,bbox_inches='tight')
    eco_figure.savefig(eco_probs_file,bbox_inches='tight')
    return (fat_probs_file,eco_probs_file)

def main(pargs,config):
    #get the users home directory
    homedir = os.path.expanduser('~')
    script_dir = os.path.dirname(os.path.abspath(__file__)) #where is this script?
    
    #Make sure grid.xml file exists
    if not os.path.isfile(pargs.gridfile):
        print('ShakeMap Grid file %s does not exist.' % pargs.gridfile)
        sys.exit(1)

    pager_folder = os.path.join(homedir,config['output_folder'])
    pager_archive = os.path.join(homedir,config['output_folder'])
    
    admin = PagerAdmin(pager_folder,pager_archive)
    
    plog = None
    if not pargs.debug:
        #stdout will now be logged as INFO, stderr will be logged as WARNING
        mail_host = config['mail_host']
        mail_from = config['mail_from']
        logfile = os.path.join(pager_folder,'pager.log')
        plog = PagerLogger(logfile,redirect=True,
                           from_address=mail_from,
                           mail_host=mail_host)
        
        plog.addEmailHandler(config['developers']) #failover email alert system

    try:
        #get all the basic event information and print it, if requested
        shake_tuple = getHeaderData(pargs.gridfile)
        eid = shake_tuple[1]['event_id']
        etime = shake_tuple[1]['event_timestamp']
        if not len(eid):
            eid = shake_tuple[0]['event_id']
        network = shake_tuple[1]['event_network']
        if network == '':
            network = 'us'
        if not eid.startswith(network):
            eid = network + eid
        

        #Create a ComcatInfo object to hopefully tell us a number of things about this event
        try:
            ccinfo = ComCatInfo(eid)
            location = ccinfo.getLocation()
            tsunami = ccinfo.getLocation()
            authid,allids = ccinfo.getAssociatedIds()
        except: #fail over to what we can determine locally
            location = shake_tuple[1]['event_description']
            tsunami = shake_tuple[1]['magnitude'] >= TSUNAMI_MAG_THRESH
            authid = eid
            allids = []
            
        #create the event directory (if it does not exist), and start logging there
        print('Creating event directory')
        event_folder = admin.createEventFolder(eid,etime)
        pager_version = get_pager_version(event_folder)
        version_folder = os.path.join(event_folder,'version.%03d' % pager_version)
        os.makedirs(version_folder)
        event_logfile = os.path.join(version_folder,'event.log')
        if plog is not None:
            plog.switchToEventFileHandler(event_logfile)

        #get the rest of the event info
        etime = shake_tuple[1]['event_timestamp']
        elat = shake_tuple[1]['lat']
        elon = shake_tuple[1]['lon']
        edepth = shake_tuple[1]['depth']
        emag = shake_tuple[1]['magnitude']
        

        #get the year of the event
        event_year = shake_tuple[1]['event_timestamp'].year

        #find the population data collected most closely to the event_year
        pop_year,popfile = _get_pop_year(event_year,config['model_data']['population_data'])

        print('Population year: %i Population file: %s\n' % (pop_year,popfile))

        #Get exposure results
        print('Calculating population exposure.')
        isofile = config['model_data']['country_grid']
        expomodel = Exposure(popfile,pop_year,isofile)
        exposure = None
        exposure = expomodel.calcExposure(pargs.gridfile)

        #get fatality results, if requested
        print('Calculating empirical fatalities.')
        fatmodel = EmpiricalLoss.fromDefaultFatality()
        fatdict = fatmodel.getLosses(exposure)

        #get economic results, if requested
        print('Calculating economic exposure.')
        econexpmodel = EconExposure(popfile,pop_year,isofile)
        ecomodel = EmpiricalLoss.fromDefaultEconomic()
        econexposure = econexpmodel.calcExposure(pargs.gridfile)
        ecodict = ecomodel.getLosses(econexposure)
        shakegrid = econexpmodel.getShakeGrid()

        #Get semi-empirical losses, if requested
        print('Calculating semi-empirical fatalities.')
        urbanfile = config['model_data']['urban_rural_grid']
        if not os.path.isfile(urbanfile):
            raise PagerException('Urban-rural grid file %s does not exist.' % urbanfile)

        semi = SemiEmpiricalFatality.fromDefault()
        semi.setGlobalFiles(popfile,pop_year,urbanfile,isofile)
        semiloss,resfat,nonresfat = semi.getLosses(pargs.gridfile)

        #get all of the other components of PAGER
        print('Getting all comments.')
        #get the fatality and economic comments
        impact1,impact2 = get_impact_comments(fatdict,ecodict,econexposure,event_year)
        #get comment describing vulnerable structures in the region.
        struct_comment = get_structure_comment(resfat,nonresfat,semi)
        #get the comment describing historic secondary hazards
        secondary_comment = get_secondary_comment(elat,elon,emag)
        #get the comment describing historical comments in the region
        historical_comment = get_historical_comment(elat,elon,emag,exposure,fatdict)

        #generate the probability plots
        print('Drawing probability plots.')
        fat_probs_file,eco_probs_file = _draw_probs(fatmodel,fatdict,
                                                    ecomodel,ecodict,
                                                    version_folder)

        #generate the exposure map
        exposure_base = os.path.join(version_folder,'exposure')
        print('Generating exposure map...')
        oceanfile = config['model_data']['ocean_vectors']
        oceangrid = config['model_data']['ocean_grid']
        cityfile = config['model_data']['city_file']
        pdf_file,png_file,mapcities = draw_contour(pargs.gridfile,popfile,
                                         oceanfile,oceangrid,cityfile,
                                         exposure_base)
        print('Generated exposure map %s' % pdf_file)
        
        
        #Create a data object to encapsulate everything we know about the PAGER
        #results, and then serialize that to disk in the form of a number of JSON files.
        print('Making PAGER Data object.')
        doc = PagerData()
        doc.setInputs(shakegrid,pager_version,shakegrid.getEventDict()['event_id'],authid,tsunami)
        print('Setting inputs.')
        doc.setExposure(exposure,econexposure)
        print('Setting exposure.')
        doc.setModelResults(fatmodel,ecomodel,
                            fatdict,ecodict,
                            semiloss,resfat,nonresfat)
        print('Setting comments.')
        doc.setComments(impact1,impact2,struct_comment,historical_comment,secondary_comment)
        print('Setting map info.')
        doc.setMapInfo(cityfile,mapcities)
        print('Validating.')
        doc.validate()
        json_folder = os.path.join(version_folder,'json')
        os.makedirs(json_folder)
        print('Saving output to JSON.')
        doc.saveToJSON(json_folder)
        print('Saving output to XML.')
        doc.saveToLegacyXML(version_folder)
        print('Done.')
    except Exception as e:
        if plog is not None:
            f = io.StringIO()
            traceback.print_exc(file=f)
            msg = errobj
            msg = '%s\n %s' % (str(msg),f.getvalue())
            hostname = socket.gethostname()
            msg = msg + '\n' + 'Error occurred on %s\n' % (hostname)
            if gridfile is not None:
                msg = msg + '\n' + 'Error on file: %s\n' % (gridfile)
            if eventcode is not None:
                msg = msg + '\n' + 'Error on event: %s\n' % (eventcode)
            if vnum is not None:
                msg = msg + '\n' + 'Error on version: %i\n' % (vnum)
            if plog is not None:
                plog.scream(msg)
                plog.stopLogging()
            print('Sent error to email')
            f.close()
        else:
            sys.stderr.write(str(e)+'\n')
            traceback.print_exc()
        sys.exit(1)
if __name__ == '__main__':
    desc = '''Run PAGER loss models and create all PAGER products.

    This program presumes that you have a configuration file in ~/.losspager/config.json,
    consisting of the following entries:

    model_data:
      population_data:
        - population_year: 1998
          population_grid: /Users/user/pager/data/lspop1998.flt
        - population_year: 2011
          population_grid: /Users/user/pager/data/lspop2011.flt
        - population_year: 2012
          population_grid: /Users/user/pager/data/lspop2012.flt
        - population_year: 2013
          population_grid: /Users/user/pager/data/lspop2013.flt


      country_grid: /Users/user/pager/data/isogrid.bil
      urban_rural_grid: /Users/user/pager/data/glurextents.bil

    database:
        url: sqlite:////Users/user/.losspager/losspager_schema.db
    
    
    Example usage:
    %(prog)s grid.xml
    '''
    parser = argparse.ArgumentParser(description=desc,formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('gridfile',
                        help='The path to a ShakeMap grid.xml file')
    parser.add_argument('-d','--debug', action='store_true',
                        default=False, help='Print debug information (mostly useful to developers)')
    parser.add_argument('-i','--eventinfo', action='store_false',
                        default=True, help='Turn off printing of basic event information')
    
    args = parser.parse_args()

    #read in the config - should be in a predictable place - this will error out if not.
    config = read_config()
    
    #Make sure model_data section exists
    try:
        config['model_data']['population_data'][0]['population_year']
        config['model_data']['population_data'][0]['population_grid']
        os.path.isfile(config['model_data']['country_grid'])
        os.path.isfile(config['model_data']['urban_rural_grid'])
    except:
        errmsg = 'Config file %s is missing some or all of the required information.  See the help for the required format.\n'
        sys.stderr.write(errmsg)
        sys.exit(1)
    
    main(args,config)
