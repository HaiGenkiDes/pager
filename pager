#!/usr/bin/env python

#stdlib imports
import argparse
import configparser
import os.path
import sys
import pprint
import locale
import json
import logging
import re
import glob
import traceback
import io
import datetime
import shutil

#third party imports
import matplotlib

#this allows us to have a non-interactive backend - essential on systems without a display
matplotlib.use('Agg')

import pandas as pd
import numpy as np
from mapio.shake import getHeaderData
from impactutils.comcat.query import ComCatInfo
from impactutils.transfer.factory import get_sender_class

#local imports
from losspager.models.exposure import Exposure
from losspager.models.econexposure import EconExposure
from losspager.models.emploss import EmpiricalLoss
from losspager.models.semimodel import SemiEmpiricalFatality
from losspager.models.growth import PopulationGrowth
from losspager.utils.country import Country
from losspager.utils.eventpath import get_event_folder
from losspager.utils.logger import PagerLogger
from losspager.utils.admin import PagerAdmin
from losspager.utils.exception import PagerException
from losspager.onepager.comment import get_impact_comments
from losspager.onepager.comment import get_structure_comment
from losspager.onepager.comment import get_secondary_comment
from losspager.onepager.comment import get_historical_comment
from losspager.onepager.onepager import create_onepager
from losspager.io.pagerdata import PagerData
from losspager.vis.impactscale import drawImpactScale
from losspager.vis.contourmap2 import draw_contour
from losspager.utils.config import read_config

COUNTRY = Country()
TIMEFMT = '%Y-%m-%d %H:%M:%S'
TSUNAMI_MAG_THRESH = 7.3



def _get_release_status(pargs,config,
                        fatmodel,fatdict,
                        ecomodel,ecodict,
                        shake_tuple,event_folder):
    #if we're not primary, it's released by default.
    #if our summary alert is green or yellow, it's released by default.
    #if the processing time is >= event time + 8(24?) hours, it's released by default.
    #if there is a "release" file in the event folder, it is released.
    #if the command line argument --release has been set, it is released.
    is_released = False

    #are we primary or secondary?
    if 'status' not in config:
        is_released = True
    else:
         if config['status'] == 'secondary':
             is_released = True

    #Are we at green or yellow
    fat_level = fatmodel.getAlertLevel(fatdict)
    eco_level = fatmodel.getAlertLevel(ecodict)
    if fat_level in ('green','yellow') and eco_level in ('green','yellow'):
        is_released = True

    #Are we past the release threshold?
    event_time = shake_tuple[1]['event_timestamp']
    threshold_hours = datetime.timedelta(seconds=config['release_threshold']*3600)
    time_threshold = event_time + threshold_hours
    if datetime.datetime.utcnow() > time_threshold:
        is_released = True

    #Is there a "release" file in the event folder?
    release_file = os.path.join(event_folder,'release')
    if os.path.isfile(release_file):
        is_released = True

    #Has the release option been set?
    if pargs.release:
        is_released = True

    return is_released

def _transfer(config,pagerdata,authid,authsource,version_folder):
    #If system status is primary and transfer options are configured, transfer the output
    #directories using those options
    if 'status' in config and config['status'] == 'primary':
        if 'transfer' in config:
            if 'methods' in config['transfer']:
                for method in config['transfer']['methods']:
                    if method not in config['transfer']:
                        sys.stderr.write('Method %s requested but not configured...Skipping.' % method)
                        continue
                    params = config['transfer'][method]
                    if 'remote_directory' in params:
                        vpath,vfolder = os.path.split(version_folder)
                        #append the event id and version folder to our pre-specified output directory
                        params['remote_directory'] = os.path.join(params['remote_directory'],eid,vfolder)
                    params['code'] = authid
                    params['eventsource'] = authsource
                    params['eventsourcecode'] = authid.replace(authsource,'')
                    params['magnitude'] = pagerdata.magnitude
                    params['latitude'] = pagerdata.latitude
                    params['longitude'] = pagerdata.longitude
                    params['depth'] = pagerdata.depth
                    params['eventtime'] = pagerdata.time
                    product_params = {'maxmmi':pagerdata.maxmmi,
                                      'alertlevel':pagerdata.summary_alert_pending}
                    sender_class = get_sender_class(method)
                    try:
                        if method == 'pdl':
                            sender = sender_class(properties=params,local_directory=version_folder,
                                                  product_properties=product_params)
                        else:
                            sender = sender_class(properties=params,local_directory=version_folder)
                        try:
                            nfiles,msg = sender.send()
                            print(msg)
                        except Exception as e:
                            print('Failed to send products via PDL: %s' % str(e))
                    except Exception as e:
                        sys.stderr.write('Could not send products via %s method - error "%s"' % (method,str(e)))
                        continue

def _get_pop_year(event_year,popyears):
    pop_year = None
    tmin = 10000000
    popfile = None
    for popdict in popyears:
        popyear = popdict['population_year']
        popgrid = popdict['population_grid']
        if not os.path.isfile(popgrid):
            print('Population grid file %s does not exist.' % popgrid)
            sys.exit(1)
        if abs(popyear-event_year) < tmin:
            tmin = abs(popyear-event_year)
            pop_year = popyear
            popfile = popgrid
    return (pop_year,popfile)

def get_pager_version(eventfolder):
    pager_version = 1
    if not os.path.isdir(eventfolder):
        os.makedirs(eventfolder)
        last_version = 0
    else:
        allfolders = glob.glob(os.path.join(eventfolder,'version.*'))
        allfolders.sort()
        if len(allfolders):
            base,last_folder = os.path.split(allfolders[-1])
            last_version = int(re.findall('\d+',last_folder)[0])
        else:
            last_version = 0
    return last_version + 1

def _draw_probs(fatmodel,fatdict,ecomodel,ecodict,version_folder):
    fatG = fatmodel.getCombinedG(fatdict)
    fat_probs = fatmodel.getProbabilities(fatdict,fatG)
    fat_figure = drawImpactScale(fat_probs,'fatality')

    ecoG = ecomodel.getCombinedG(ecodict)
    eco_probs = ecomodel.getProbabilities(ecodict,ecoG)
    eco_figure = drawImpactScale(eco_probs,'economic')

    fat_probs_file = os.path.join(version_folder,'alertfatal.pdf')
    eco_probs_file = os.path.join(version_folder,'alertecon.pdf')
    fat_figure.savefig(fat_probs_file,bbox_inches='tight')
    eco_figure.savefig(eco_probs_file,bbox_inches='tight')
    return (fat_probs_file,eco_probs_file)

def main(pargs,config):
    #get the users home directory
    homedir = os.path.expanduser('~')
    script_dir = os.path.dirname(os.path.abspath(__file__)) #where is this script?
    
    #Make sure grid.xml file exists
    if not os.path.isfile(pargs.gridfile):
        print('ShakeMap Grid file %s does not exist.' % pargs.gridfile)
        sys.exit(1)

    pager_folder = os.path.join(homedir,config['output_folder'])
    pager_archive = os.path.join(homedir,config['output_folder'])
    
    admin = PagerAdmin(pager_folder,pager_archive)
    
    plog = None
    if not pargs.debug:
        #stdout will now be logged as INFO, stderr will be logged as WARNING
        mail_host = config['mail_host']
        mail_from = config['mail_from']
        logfile = os.path.join(pager_folder,'pager.log')
        plog = PagerLogger(logfile,redirect=True,
                           from_address=mail_from,
                           mail_host=mail_host)
        
        plog.addEmailHandler(config['developers']) #failover email alert system

    try:
        #get all the basic event information and print it, if requested
        shake_tuple = getHeaderData(pargs.gridfile)
        eid = shake_tuple[1]['event_id']
        etime = shake_tuple[1]['event_timestamp']
        if not len(eid):
            eid = shake_tuple[0]['event_id']
        network = shake_tuple[1]['event_network']
        if network == '':
            network = 'us'
        if not eid.startswith(network):
            eid = network + eid
        

        #Create a ComcatInfo object to hopefully tell us a number of things about this event
        try:
            ccinfo = ComCatInfo(eid)
            location = ccinfo.getLocation()
            tsunami = ccinfo.getTsunami()
            authid,allids = ccinfo.getAssociatedIds()
            authsource,othersources = ccinfo.getAssociatedSources()
        except: #fail over to what we can determine locally
            location = shake_tuple[1]['event_description']
            tsunami = shake_tuple[1]['magnitude'] >= TSUNAMI_MAG_THRESH
            authid = eid
            authsource = network
            allids = []

        #Check to see if user wanted to override default tsunami criteria
        if pargs.tsunami != 'auto':
            if pargs.tsunami == 'on':
                tsunami = True
            else:
                tsunami = False
            
        #check to see if this event is a scenario
        shakemap_type = shake_tuple[0]['shakemap_event_type']
        if shakemap_type == 'SCENARIO':
            if re.search('scenario',location.lower()) is None:
                location = 'Scenario '+location
            
        #create the event directory (if it does not exist), and start logging there
        print('Creating event directory')
        event_folder = admin.createEventFolder(authid,etime)
        pager_version = get_pager_version(event_folder)
        version_folder = os.path.join(event_folder,'version.%03d' % pager_version)
        os.makedirs(version_folder)
        event_logfile = os.path.join(version_folder,'event.log')
        if plog is not None:
            plog.switchToEventFileHandler(event_logfile)

        #Check to see if the tsunami flag has been previously set
        tsunami_toggle = {'on':1,'off':0}
        tsunami_file = os.path.join(event_folder,'tsunami')
        if os.path.isfile(tsunami_file):
            tsunami = tsunami_toggle[open(tsunami_file,'rt').read().strip()]
            
        #get the rest of the event info
        etime = shake_tuple[1]['event_timestamp']
        elat = shake_tuple[1]['lat']
        elon = shake_tuple[1]['lon']
        edepth = shake_tuple[1]['depth']
        emag = shake_tuple[1]['magnitude']
        

        #get the year of the event
        event_year = shake_tuple[1]['event_timestamp'].year

        #find the population data collected most closely to the event_year
        pop_year,popfile = _get_pop_year(event_year,config['model_data']['population_data'])

        print('Population year: %i Population file: %s\n' % (pop_year,popfile))

        #Get exposure results
        print('Calculating population exposure.')
        isofile = config['model_data']['country_grid']
        expomodel = Exposure(popfile,pop_year,isofile)
        exposure = None
        exposure = expomodel.calcExposure(pargs.gridfile)

        #get fatality results, if requested
        print('Calculating empirical fatalities.')
        fatmodel = EmpiricalLoss.fromDefaultFatality()
        fatdict = fatmodel.getLosses(exposure)

        #get economic results, if requested
        print('Calculating economic exposure.')
        econexpmodel = EconExposure(popfile,pop_year,isofile)
        ecomodel = EmpiricalLoss.fromDefaultEconomic()
        econexposure = econexpmodel.calcExposure(pargs.gridfile)
        ecodict = ecomodel.getLosses(econexposure)
        shakegrid = econexpmodel.getShakeGrid()

        #Get semi-empirical losses, if requested
        print('Calculating semi-empirical fatalities.')
        urbanfile = config['model_data']['urban_rural_grid']
        if not os.path.isfile(urbanfile):
            raise PagerException('Urban-rural grid file %s does not exist.' % urbanfile)

        semi = SemiEmpiricalFatality.fromDefault()
        semi.setGlobalFiles(popfile,pop_year,urbanfile,isofile)
        semiloss,resfat,nonresfat = semi.getLosses(pargs.gridfile)

        #get all of the other components of PAGER
        print('Getting all comments.')
        #get the fatality and economic comments
        impact1,impact2 = get_impact_comments(fatdict,ecodict,econexposure,event_year)
        #get comment describing vulnerable structures in the region.
        struct_comment = get_structure_comment(resfat,nonresfat,semi)
        #get the comment describing historic secondary hazards
        secondary_comment = get_secondary_comment(elat,elon,emag)
        #get the comment describing historical comments in the region
        historical_comment = get_historical_comment(elat,elon,emag,exposure,fatdict)

        #generate the probability plots
        print('Drawing probability plots.')
        fat_probs_file,eco_probs_file = _draw_probs(fatmodel,fatdict,
                                                    ecomodel,ecodict,
                                                    version_folder)

        #generate the exposure map
        exposure_base = os.path.join(version_folder,'exposure')
        print('Generating exposure map...')
        oceanfile = config['model_data']['ocean_vectors']
        oceangrid = config['model_data']['ocean_grid']
        cityfile = config['model_data']['city_file']
        pdf_file,png_file,mapcities = draw_contour(pargs.gridfile,popfile,
                                         oceanfile,oceangrid,cityfile,
                                         exposure_base)
        print('Generated exposure map %s' % pdf_file)

        #figure out whether this event has been "released".
        is_released = _get_release_status(pargs,config,
                                          fatmodel,fatdict,
                                          ecomodel,ecodict,
                                          shake_tuple,event_folder)
        
        
        
        #Create a data object to encapsulate everything we know about the PAGER
        #results, and then serialize that to disk in the form of a number of JSON files.
        print('Making PAGER Data object.')
        doc = PagerData()
        timezone_file = config['model_data']['timezones_file']
        doc.setInputs(shakegrid,timezone_file,pager_version,shakegrid.getEventDict()['event_id'],
                      authid,tsunami,location,is_released)
        print('Setting inputs.')
        doc.setExposure(exposure,econexposure)
        print('Setting exposure.')
        doc.setModelResults(fatmodel,ecomodel,
                            fatdict,ecodict,
                            semiloss,resfat,nonresfat)
        print('Setting comments.')
        doc.setComments(impact1,impact2,struct_comment,historical_comment,secondary_comment)
        print('Setting map info.')
        doc.setMapInfo(cityfile,mapcities)
        print('Validating.')
        doc.validate()
        json_folder = os.path.join(version_folder,'json')
        os.makedirs(json_folder)
        print('Saving output to JSON.')
        doc.saveToJSON(json_folder)
        print('Saving output to XML.')
        doc.saveToLegacyXML(version_folder)

        print('Creating onePAGER pdf...')
        onepager_pdf,error = create_onepager(doc,version_folder)
        if onepager_pdf is None:
            raise PagerException('Could not create onePAGER output: \n%s' % error)
        if plog is not None:
            plog.stopLogging()
        print('Created onePAGER pdf %s' % onepager_pdf)

        #copy the contents.xml file to the version folder
        contentsfile = os.path.join(script_dir,'losspager','data','contents.xml')
        shutil.copy(contentsfile,version_folder)
        
        #run transfer, as appropriate and as specified by config
        print('Running transfer (as appropriate).')
        _transfer(config,doc,authid,authsource,version_folder)
        
        print('Done.')
    except Exception as e:
        if plog is not None:
            f = io.StringIO()
            traceback.print_exc(file=f)
            msg = errobj
            msg = '%s\n %s' % (str(msg),f.getvalue())
            hostname = socket.gethostname()
            msg = msg + '\n' + 'Error occurred on %s\n' % (hostname)
            if gridfile is not None:
                msg = msg + '\n' + 'Error on file: %s\n' % (gridfile)
            if eventcode is not None:
                msg = msg + '\n' + 'Error on event: %s\n' % (eventcode)
            if vnum is not None:
                msg = msg + '\n' + 'Error on version: %i\n' % (vnum)
            if plog is not None:
                plog.scream(msg)
                plog.stopLogging()
            print('Sent error to email')
            f.close()
        else:
            sys.stderr.write(str(e)+'\n')
            traceback.print_exc()
        sys.exit(1)
if __name__ == '__main__':
    desc = '''Run PAGER loss models and create all PAGER products.

    This program presumes that you have a configuration file in ~/.losspager/config.json,
    consisting of the following entries:

    model_data:
      population_data:
        - population_year: 1998
          population_grid: /Users/user/pager/data/lspop1998.flt
        - population_year: 2011
          population_grid: /Users/user/pager/data/lspop2011.flt
        - population_year: 2012
          population_grid: /Users/user/pager/data/lspop2012.flt
        - population_year: 2013
          population_grid: /Users/user/pager/data/lspop2013.flt


      country_grid: /Users/user/pager/data/isogrid.bil
      urban_rural_grid: /Users/user/pager/data/glurextents.bil

    database:
        url: sqlite:////Users/user/.losspager/losspager_schema.db
    
    
    Example usage:
    %(prog)s grid.xml
    '''
    parser = argparse.ArgumentParser(description=desc,formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('gridfile',
                        help='The path to a ShakeMap grid.xml file')
    parser.add_argument('-d','--debug', action='store_true',
                        default=False, help='Print debug information (mostly useful to developers)')
    parser.add_argument('-i','--eventinfo', action='store_false',
                        default=True, help='Turn off printing of basic event information')
    parser.add_argument('--release', action='store_true',
                        default=False, help='Set release status to True (override other criteria)')
    parser.add_argument('--tsunami', choices=['on','off','auto'],
                        default='auto', help='Set tsunami flag to True/False (override other criteria), or use default criteria')
    
    args = parser.parse_args()

    #read in the config - should be in a predictable place - this will error out if not.
    config = read_config()
    
    #Make sure model_data section exists
    try:
        config['model_data']['population_data'][0]['population_year']
        config['model_data']['population_data'][0]['population_grid']
        os.path.isfile(config['model_data']['country_grid'])
        os.path.isfile(config['model_data']['urban_rural_grid'])
    except:
        errmsg = 'Config file %s is missing some or all of the required information.  See the help for the required format.\n'
        sys.stderr.write(errmsg)
        sys.exit(1)
    
    main(args,config)
